---
title: "STA 380 Exercises"
author: "Sujay Chebbi, India Lindsay, Mauricio Morales, Matthew Streichler"
date: "8/17/2020"
output:
  md_document:
    variant: markdown_github
---
```{r setup, include=FALSE, warnings=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#loading required packages
library(ggplot2)
library(gridExtra)

```

# Exercise 1: Visual Story Telling Part 1 - Green Buildings

```{r, include=FALSE, warnings=FALSE}
Green = read.csv('C:/Users/india/Documents/GitHub/STA380/data/greenbuildings.csv')

#Count of the number of buildings with occupancy rate below 10%
green2 = dim(Green[Green$leasing_rate <= 10,])[1]
Green = Green[Green$leasing_rate > 10,] #and dropping them 


greenyes = Green[Green$green_rating==1,] #green buildings
greenno = Green[Green$green_rating==0,] #non green buildings
nyes = dim(greenyes)[1] #number of green buildings
nno = dim(greenno[1]) #number of non green buildings
```

  The Austin-based real estate developer is seeking to understand the financial returns to green certification for her new building. Construction of the building is priced at $100 million, with an expected $5 million premium for green certification. An analyst on the developer's staff estimated that green certification for the building will result in an additional $650,000 of yearly revenue and that it will take 7.7 years to recuperate the premium, considering the building has an occupancy rate of 100%. The analyst calculated his estimates by looking at all buildings with leasing rates greater than 10%. The goal of our analysis is to revisit the analyst's estimates, using the same data set containing statistics on 7,894 buildings, and determine if the presented returns to green certification are accurate.
  
  
### Newer, Taller, Larger Buildings Have Higher Rental Rates

  The developer's building is 15 stories tall, 250,000 square feet, and has an age of 0. To evaluate if the analyst should have accounted for our building's physical features, the below plots reveal the general relationships between Rent and each of these features. The gray shading around the line represents our margin of error for this estimate and the vertical red line represents our building.
  

```{r, echo=FALSE, warnings=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
ages = ggplot(Green, aes(x=age, y=Rent)) + geom_smooth(formula = y ~ s(x, bs = "cs"), method='gam', se = TRUE) +
  geom_vline(xintercept=0,linetype="dashed", color = "red") +
  labs(title="Age vs Rent",
    subtitle = "Newer Buildings Rent For More",
    x = "Age of Buildings", y="Yearly Rent Per Square Foot"
  )

#geom_point(aes(age,Rent,color=green_rating))

stories = ggplot(Green, aes(stories, Rent)) + geom_vline(xintercept=15,linetype="dashed", color = "red") +
  geom_smooth(formula = y ~ s(x, bs = "cs"), method='gam', se = TRUE) +
  labs(
    title = "Stories vs Rent",
    subtitle = "Taller Buildings Rent For More", caption="(for buildings less than ~75 stories)",
    x = "Number of Stories", y="Yearly Rent Per Square Foot"
  )

size = ggplot(Green, aes(size, Rent)) + geom_vline(xintercept=250000,linetype="dashed", color = "red") +
  geom_smooth(formula = y ~ s(x, bs = "cs"), method='gam', se = TRUE) +
  labs(title='Size vs Rent',
    subtitle = "Larger Buildings Rent For More",
    x = "Building Size", y="Yearly Rent Per Square Foot"
  )

grid.arrange(ages,stories,size,ncol=3)
```
 
### Location, Location, and... Location

 The building is located on East Cesar Chavez in Austin, Texas. Using outside sources, we were able to identify the local employment growth rate (2.5% as of March 31, 2020: [source](https://www.austinchamber.com/blog/04-21-2020-job-growth-unemployment)), average annual rainfall (34 inches: [source](https://www.weather.gov/media/ewx/climate/ClimateSummary-ewx-Austin.pdf)), and the number of degree days (2,248 heating days, 2,862 cooling days, and 5,110 total degree days: [source](https://www.eia.gov/energyexplained/units-and-calculators/degree-days.php)). The data set measured the values for degree days in 2007 however, as the 2007 estimates were unavailable, we used the 2018 values as they are likely a close estimate. We were unable to find information on the other location-based variables present in the data set.
 
  The below plots investigate whether these geographical characteristics have any affect on a building's rent. The red dashed line represents the data set's mean rent. While the direct relationship between the following location-based features and rent may not be clear, they help us understand where the rental market is inflated. 
 
```{r, echo=FALSE, warnings=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}

P <- ggplot(Green, aes(x=Rent,Precipitation)) + geom_smooth(formula = y ~ s(x, bs = "cs"), method='gam', se = TRUE) + 
  geom_hline(yintercept=mean(Green$Rent), linetype="dashed", color = "red") +
  labs(
    title = "Precipitation vs Rent", subtitle="Wetter Cities Have Higher Rent",
    x = "Annual Rainfall (inches)", y="Yearly Rent Per Square Foot"
  )

E <- ggplot(Green, aes(x=empl_gr,Rent)) + geom_smooth(formula = y ~ s(x, bs = "cs"), method='gam', se = TRUE) + geom_hline(yintercept=mean(Green$Rent), linetype="dashed", color = "red") +
  labs(
    title = "Employment Growth Rate vs Rent", subtitle="Negative Rates: Rent Fluctuates, Positive Rates: Rent More Stable",
    x = "Employment Rate", y="Yearly Rent Per Square Foot"
  ) + theme(plot.subtitle=element_text(size=8))

grid.arrange(E,P, ncol=2)

```

The left plot indicates how rent is only stable for regions with positive employment growth rates. Otherwise, rent greatly varies for different levels of employment growth rates. The right plot reveals how rent spikes for regions with greater precipitation. This graph may be capturing the high rental rates for cities that receive high rainfall, such as San Francisco and the Pacific Northwest region. 


```{r, echo=FALSE, warnings=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}

D <- ggplot(Green, aes(x=total_dd_07,Rent)) + geom_smooth(formula = y ~ s(x, bs = "cs"), method='gam', se = TRUE)  + geom_hline(yintercept=mean(Green$Rent), linetype="dashed", color = "red") +
  labs(
    title = "Total Degree Days vs Rent", caption = "Total Degree Days Accounts For Both Cooling And Heating Days",
    x = "Total Degree Days", y="Yearly Rent Per Square Foot"
  ) 
D


```
In the above plot, the big spike that occurs around 6,000 degree days is likely capturing cities with more extreme temperatures; NYC, Chicago, or Miami. The high left values with more moderate-climates may represent the Pacific Northwest region. The dip in the lower right might capture northern regions with very extreme weather and low property values. 

The relationship between rent and regional variables is complicated. Ultimately, it is best to account for as many physical and regional features as possible when estimating returns to green certification.


### Taking A Relevant Subset

  We obtained a subset of the buildings data set that contained buildings within +/- 1 standard deviations of our building's size, stories, age, regional employment growth rate, regional annual rainfall, and regional degree days. We dropped buildings that had less than 10% occupancy to avoid outliers. This data set contained information on 314 buildings, 73 of which were green and 241 of which were non-green.
  

```{r, include=FALSE, warnings=FALSE}
#getting SD of our variables
sd(Green$size) #297,533.4
sd(Green$stories) #12.287
sd(Green$age) #32
sd(Green$empl_gr) #NA? 
emp = na.omit(Green$empl_gr)
sd(emp) #8.163
sd(Green$hd_total07) #1976.937
sd(Green$cd_total_07) #1104.589
sd(Green$Precipitation) #11.575

#creating subset
new_green = subset(Green, subset=((size<550000) & (stories>3) & (stories<27) & (age<32) & (empl_gr<8) & (Precipitation>23) & (Precipitation < 45) & (hd_total07>200) & (hd_total07 < 5200) & (cd_total_07>1700) & (cd_total_07 < 3900)))

#distinguishing between green vs non-green
newgreenyes = new_green[new_green$green_rating==1,]
newgreenno = new_green[new_green$green_rating==0,]

summary(newgreenyes$Rent)
summary(newgreenno$Rent)

```
  
### What Can We Learn From Buildings With Similar Features? 

  Looking only at similar buildings, the median rent for a green building is $22 while the median rent for a non-green building is $20.25. 


```{r, echo=FALSE, warnings=FALSE, fig.height = 3, fig.width = 7, fig.align='center'}
n = dim(new_green)[1]
ggplot(new_green, aes(x=Rent,y=factor(green_rating))) + geom_boxplot() +
  theme(axis.text.x = element_text(angle=65, vjust=0.6)) + scale_y_discrete(labels=c("0" = "Non-Green", "1" = "Green")) + 
  labs(title="Green Buildings Have a Higher Average Rent", subtitle="Offering an Avg $1.75 More Per Sq. Ft.", y="Type of Building", x="Yearly Rent Per Square Foot")

```

  Green buildings also have a higher median leasing rate of 91.43% compared with the non-green leasing rate of 88.49%. 

```{r, echo=FALSE, warnings=FALSE, fig.height = 3, fig.width = 7, fig.align='center'}
#density plot for leasing rate
ggplot(data=new_green, aes(x=leasing_rate)) + geom_density(aes(fill=factor(green_rating))) + scale_fill_discrete(name = "Type of Building", labels = c("Non-Green","Green")) +
   labs(title="Green Buildings Tend to Have Higher Leasing Rates", x="Leasing Rate", y="Proportion of Buildings") 
```
 
### Other Lurking Features Remain 

The building's quality is measured in the data set by standards of high (class a), average (class b), or low (class c). The below plots reveal how the quality of the building has a significant association with higher rent per sq foot for buildings similar to ours. One would expect green certified buildings to be of higher quality however, these variables have a fairly low correlation of 0.248. 

```{r, echo=FALSE, warnings=FALSE, fig.height = 3, fig.width = 7, fig.align='center'}
#G_amenities = ggplot(newgreenyes, aes(x=Rent,y=factor(amenities))) + geom_boxplot() + scale_y_discrete(labels=c("0" = "Low Standard", "1" = "High Standard")) +  labs(title="Rent for Green Buildings By Standard of Amenities", subtitle="Avg Rent Per Sq Ft is $0.65 Greater for High Standard of Amenities", x="Leasing Rate", y="Standard of Amenities") 

#N_amenities = ggplot(newgreenno, aes(x=Rent,y=factor(amenities))) + geom_boxplot() + scale_y_discrete(labels=c("0" = "Low Standard", "1" = "High Standard")) +  labs(title="Rent for Non Green Buildings By Standard of Amenities", x="Leasing Rate", y="Standard of Amenities") 

#nice_amen = subset(newgreenyes, subset=((amenities==1)))
#median(nice_amen$Rent) #avg rent is 22

#not_amen = subset(newgreenyes, subset=((amenities==0)))
#median(not_amen$Rent) #avg rent is 21.69

#grid.arrange(G_amenities,N_amenities ,ncol=2)

G_class = ggplot(newgreenyes, aes(x=Rent,y=factor(class_a))) + geom_boxplot()  + scale_y_discrete(labels=c("0" = "Average or Low Quality", "1" = "Highest Quality")) +  labs(title="Rent for Green Buildings By Building Quality", subtitle="Avg Rent Per Sq Ft is $4 Greater for High Building Quality", x="Leasing Rate", y="Building Quality")

nice_class = subset(newgreenyes, subset=((class_a==1)))
avg_nice = median(nice_class$Rent) #avg rent is 23

low_class = subset(newgreenyes, subset=((class_a==0)))
avg_low = median(low_class$Rent) #avg rent is 19

N_class = ggplot(newgreenno, aes(x=Rent,y=factor(class_a))) + geom_boxplot()  + scale_y_discrete(labels=c("0" = "Average or Low Quality", "1" = "Highest Quality")) +  labs(title="Rent for Non-Green Buildings By Building Quality", subtitle="Avg Rent Per Sq Ft is $3 Greater for High Building Quality", x="Leasing Rate", y="Building Quality")

Nnice_class = subset(newgreenno, subset=((class_a==1)))
Navg_nice = median(Nnice_class$Rent) #avg rent is 23

Nlow_class = subset(newgreenno, subset=((class_a==0)))
Navg_low = median(Nlow_class$Rent) #avg rent is 19

G_class
N_class

cor_stat = cor(Green$green_rating,Green$class_a)
```
 

### Concluding Thoughts 

  The analyst's conclusion that a green certification yields a greater return was accurate however, his analysis was quite faulty. He greatly overestimated the degree of the return in yearly rent per square foot as he did not account for the building's characteristics. The analyst focused on the number of years it would take to recuperate the initial premium. As it is almost impossible that every individual square foot of a building could be rented out, these estimates are irrelevant. 
  
  Our analysis concluded that green certification is worth the extra premium. Green buildings offer an additional $1.75 in yearly rent per square foot, on average. Based upon higher average occupancy rates, it is more likely that green buildings will have a higher total level of income than non-green buildings. However, it is essential that the developer ensure her building is of high quality as this variable leads to a greater difference in rent for buildings similar to ours.
  
# Exercise 2: Visual Story Telling Part 2 - Flights at ABIA

Our task is to create a figure, or set of related figures, that tell an interesting story about flights into and out of Austin.
```{r reading2, include=FALSE}
library(mosaic)
library(tidyverse)
library(ggplot2)
ABIA = read.csv('C:/Users/india/Documents/GitHub/STA380/data/ABIA.csv')
attach(ABIA)
```

### Plots


March is the month with the most canceled flights.
```{r plot12, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
ggplot(data = ABIA) + 
  geom_bar(mapping = aes(x=Month, y=Cancelled), stat = "identity") + labs(title = "2008 Total cancelled flights per month", y="Cancelled flights", color="Month")
```


Most canceled flights are due to carrier or weather. If we compare to the previous plot, March cancellations are mostly because of weather. However, the second month to have the most cancellations is April, which cancellations are mostly due to carrier.
```{r plot22, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
s = subset(ABIA, (CancellationCode == "A" | CancellationCode == "B" | CancellationCode == "C" | CancellationCode == "D"))
ggplot(data = s) + 
  geom_bar(mapping = aes(x=Month, y=Cancelled), stat = "identity") + labs(title = "2008 cancelled flights due to A = Carrier, B = Weather, C = NAS", y="Cancelled flights", color="Month") + 
  facet_grid((CancellationCode)~.)
```


Considering all of the cancellations of the year. It seems that it's not good idea to take flights on the second quarter of the month.
```{r plot32, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
ggplot(data = ABIA) + 
  geom_bar(mapping = aes(x=DayofMonth, y=Cancelled), stat = "identity") + labs(title = "2008 Total cancelled flights per day of the month", y="Cancelled flights", color="Month")
```


However, if we look at the cancellations per month per day, September and April are the months that are causing this spike.
```{r plot322, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
ggplot(data = ABIA) + 
  geom_bar(mapping = aes(x=DayofMonth, y=Cancelled), stat = "identity") + labs(title = "2008 Cancelled flights per month per day", y="Cancelled flights", color="Month") +
  facet_wrap(~ Month, nrow = 2)
```


Tuesday seems to be a bad choice to travel if we consider the total cancellations of the year in that day.
```{r plot42, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
ggplot(data = ABIA) + 
  geom_bar(mapping = aes(x=DayOfWeek, y=Cancelled), stat = "identity") + labs(title = "2008 Total cancelled flights per day of the week", y="Cancelled flights", color="Month")
```


Looking at the plot of canceled flights per month per day of the week. We still see that Tuesday is the day with most canceled flights for mostly of the months, being March and April the months with the higher amount of cancellations on Tuesday.
```{r plot422, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
ggplot(data = ABIA) + 
  geom_bar(mapping = aes(x=DayOfWeek, y=Cancelled), stat = "identity") + labs(title = "2008 Cancelled flights per month per day of the week", y="Cancelled flights", color="Month") +
  facet_wrap(~ Month, nrow = 2)
```


Austin is the origin and destination with the most canceled flights. It was taken out of the plot because the amount of canceled flights from Austin is more than 4 times the amount of canceled flights of Dallas, which made the other values seem irrelevant in magnitude.

DFW is the second origin and destination with the most canceled flights.
```{r plot52, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
ggplot(data = subset(ABIA, (ABIA$Cancelled > 0 & ABIA$Dest != "AUS")), aes(x=reorder(Dest,Cancelled, FUN=sum),y=Cancelled)) +
  geom_bar(stat = "identity") +
  labs(title = "2008 Cancelled flights per destination", x = "Destination", y = "Cancelled flights") +
  coord_flip()
```

```{r plot62, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
ggplot(data = subset(ABIA, (ABIA$Cancelled > 0 & ABIA$Origin != "AUS")), aes(x=reorder(Origin,Cancelled, FUN=sum),y=Cancelled)) +
  geom_bar(stat = "identity") +
  labs(title = "2008 Cancelled flights per origin", x = "Origin", y = "Cancelled flights") +
  coord_flip()
```


If we compare previous plots with the total amount of flights per Origin, the reason why there more cancellations in Austin and Dallas may be due to the amount of flights in those places rather than an external value. For this plot, Austin was also taken out, because the magnitude made the other values seem irrelevant.
```{r plot72, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
j <- within(subset(ABIA, (ABIA$Cancelled > 0 & ABIA$Origin != "AUS")), Origin <- factor(Origin, levels=names(sort(table(Origin), decreasing=FALSE))))
ggplot(j,aes(x=Origin))+geom_bar() + coord_flip() + labs(title = "2008 Total flights per origin", x = "Origin", y = "Total flights")
```

# Exercise 3: Portfolio Modeling

Clear global environment and load in necessary libraries. Setting seed to keep numbers consistent.
```{r, include=FALSE}
rm(list=ls())
library(mosaic)
library(quantmod)
library(foreach)
library(ggplot2)
set.seed(1234)
```


### Portfolio 1

Load in stocks that will be included in the portfolio. Make sure that they are gathering data for the past 5 years. Adjusted the stock prices to account for splits, dividends, etc. -- convert to Adjusted Closing Prices.
```{r, include=FALSE}
#Invesco QQQ Trust
#SPDR S&P500 Trust ETF
#Vanguard S&P500 ETF
#SPDR Gold Shares ETF
mystocks1 = c("QQQ", "SPY", "VOO", "GLD")
myprices1 = getSymbols(mystocks1, from = "2015-08-07")


# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
#for every ticker, adjust for splits, dividends, etc.
for(ticker in mystocks1) {
  expr1 = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr1))
}
```


These are the variations of returns and top few returns within each of the stocks in the portfolio.
```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
# Look at close-to-close changes -- these are the returns
plot(ClCl(QQQa), main="Adjusted Price Stock Chart - QQQ", xlab="Date", ylab="Return (%)")
plot(ClCl(SPYa), main="Adjusted Price Stock Chart - SPY", xlab="Date", ylab="Return (%)")
plot(ClCl(VOOa), main="Adjusted Price Stock Chart - VOO", xlab="Date", ylab="Return (%)")
plot(ClCl(GLDa), main="Adjusted Price Stock Chart - GLD", xlab="Date", ylab="Return (%)")

# Combine all the returns in a matrix
all_returns1 = cbind(ClCl(QQQa), ClCl(SPYa), ClCl(VOOa), ClCl(GLDa))
head(all_returns1)
```

Returns are calculated by (Price1 - Price0)/Price0. Due to there being no previous data, the first entry has "NA" as the return. This code block removes this from the data set so that results are more accurate.
```{r, echo=FALSE}
# first row is NA because we didn't have a "before" in our data
all_returns1 = as.matrix(na.omit(all_returns1))
N1 = nrow(all_returns1)
```

This shows the pairwise plots (correlation matrix) between all the stocks within the portfolio. Based on the plots, it is shown that QQQ, SPY, and VOO all have a high positive correlation with one another. SPY and VOO have a very high correlation, something that looks close to +1. This is probably due to VOO and SPY both being ETFs drawn from the S&P 500 index.
```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
# These returns can be viewed as draws from the joint distribution
# strong correlation, but certainly not Gaussian!  
pairs(all_returns1)
```

Simulating through 5000 iterations of possible returns of the portfolio.
```{r, include=FALSE}
# Now simulate many different possible futures
# just repeating the above block thousands of times
initial_wealth1 = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth1 = initial_wealth1
  weights1 = c(0.25, 0.25, 0.25, 0.25)
  holdings1 = weights1 * total_wealth1
  n_days = 20
  wealthtracker1 = rep(0, n_days)
  for(today in 1:n_days) {
    return.today1 = resample(all_returns1, 1, orig.ids=FALSE)
    holdings1 = holdings1 + holdings1*return.today1
    total_wealth1 = sum(holdings1)
    wealthtracker1[today] = total_wealth1
  }
  wealthtracker1
}
```

USD value of the possible returns through the 5000 iterations. Looking at the histogram, this portfolio seems to be profitable as the average return is greater than $100,000.
```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
# each row is a simulated trajectory
# each column is a data
head(sim1)
hist(sim1[,n_days], 25, main="Average Returns - Portfolio 1", xlab="Amount Returned ($USD)", ylab="Frequency")   #give me 25 bins in the histograms
```

This outputs the average USD amount one can receive from this portfolio as well as the average USD amount of the profit/loss yielded. The histogram outputs the range of possible profits/losses. The mean yield of this portfolio is $101,162.20 which returns a profit of $1,162.19 This is backed up by the histogram as the most likely scenarios shown is above $0 which tells us that the portfolio yields a profitable return.
```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
# Profit/loss
mean(sim1[,n_days])      #How much money I return from this portfolio and weights
mean(sim1[,n_days] - initial_wealth1)      #How much profit I made from this portfolio
hist(sim1[,n_days]- initial_wealth1, breaks=30, main="Average Profit/Loss - Portfolio 1", xlab="Profit/Loss Returned ($USD)", ylab="Frequency")    #distribution of profit/loss

```

Outputs the 5% value at risk as well as the greatest and least possible value at risks with this iteration.
```{r, echo=FALSE}
# 5% value at risk:
quantile(sim1[,n_days]- initial_wealth1, prob=0.05)
#5% likely to to have a value $5,632.39 lower than the amount I put into my portfolio

#99.999999999999999% value at risk:
quantile(sim1[,n_days]- initial_wealth1, prob=0.99999999999999999)
#00.000000000000001% value at risk:
quantile(sim1[,n_days]- initial_wealth1, prob=0.00000000000000001)

```

### Analysis

We decided to pick QQQ, SPY, VOO, and GLD as they are all very popular ETFs. If you buy/sell securities, then you most likely have one, if not multiple, ETFs that we included. We decided to go with these 4 rather than a safe ETF, an aggressive one, a diverse one, and some fourth one as we wanted to mimic what one might really have in their personal portfolio. Based on the 4 securities that are included and the results from the code, it is seen that the average profit one will return with an equally weighted portfolio will typically be positive. However, there is possibility that this portfolio can return as much as $29,904.46 and lose as most as $17,857.80 (both very unlikely). The VaR of this portfolio is valued at $5,966.09 meaning that at the 5% confidence level, the portfolio will lose $5,966.09. Each time the simulation runs, these VaR levels change the amount one will return.


### Portfolio 2

Load in stocks that will be included in the portfolio. Make sure that they are gathering data for the past 5 years. Adjusted the stock prices to account for splits, dividends, etc. -- convert to Adjusted Closing Prices.
```{r, include=FALSE}
#Commodity ETFs
#Invesco Optimum Yield Diversified Commodity Strategy No K-1 ETF
#Inesco DB Commodity Index Tracking Fund
#iShares S&P GSCI Commodity-Indexed Trust
#iPath Dow Jones-UBS Commodity ETN
#United States Commodity Index Fund
#RICI-Total Return ETN
mystocks2 = c("PDBC", "DBC", "GSG", "DJP", "USCI", "RJI")
myprices2 = getSymbols(mystocks2, from = "2015-08-07")


# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
#for every ticker, adjust for splits, dividends, etc.
for(ticker in mystocks2) {
  expr2 = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr2))
}
```

These are the variations of returns and top few returns within each of the stocks in the portfolio.
```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
# Look at close-to-close changes -- these are the returns
plot(ClCl(PDBCa), main="Adjusted Price Stock Chart - PDBC", xlab="Date", ylab="Return (%)")
plot(ClCl(DBCa), main="Adjusted Price Stock Chart - DBC", xlab="Date", ylab="Return (%)")
plot(ClCl(GSGa), main="Adjusted Price Stock Chart - GSG", xlab="Date", ylab="Return (%)")
plot(ClCl(DJPa), main="Adjusted Price Stock Chart - DJPA", xlab="Date", ylab="Return (%)")
plot(ClCl(USCIa), main="Adjusted Price Stock Chart - USCI", xlab="Date", ylab="Return (%)")
plot(ClCl(RJIa), main="Adjusted Price Stock Chart - RJI", xlab="Date", ylab="Return (%)")

# Combine all the returns in a matrix
all_returns2 = cbind(ClCl(PDBCa), ClCl(DBCa), ClCl(GSGa), ClCl(DJPa), ClCl(USCIa), ClCl(RJIa))
head(all_returns2)
```

Returns are calculated by (Price1 - Price0)/Price0. Due to there being no previous data, the first entry has "NA" as the return. This code block removes this from the data set so that results are more accurate.
```{r, echo=FALSE}
# first row is NA because we didn't have a "before" in our data
all_returns2 = as.matrix(na.omit(all_returns2))
N2 = nrow(all_returns2)
```

This shows the pairwise plots (correlation matrix) between all the stocks within the portfolio. Based on the correlation plots, it is seen that all stocks within portfolio 2 have a positive correlation with each other. USCI does not have as high of correlation with any individual stocks compared to the other 5 stocks. This can be seen as there is more scatter in the plots that include USCI.
```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
# These returns can be viewed as draws from the joint distribution
# strong correlation, but certainly not Gaussian!  
pairs(all_returns2)
```

Simulating through 5000 iterations of possible returns of the portfolio.
```{r, include=FALSE}
# Now simulate many different possible futures
# just repeating the above block thousands of times
initial_wealth2 = 100000
sim2 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth2 = initial_wealth2
  weights2 = c(0.15, 0.2, 0.05, 0.4, 0.1, 0.1)
  holdings2 = weights2 * total_wealth2
  n_days = 20
  wealthtracker2 = rep(0, n_days)
  for(today in 1:n_days) {
    return.today2 = resample(all_returns2, 1, orig.ids=FALSE)
    holdings2 = holdings2 + holdings2*return.today2
    total_wealth2 = sum(holdings2)
    wealthtracker2[today] = total_wealth2
  }
  wealthtracker2
}
```

USD value of the possible returns through the 5000 iterations. Looking at the histogram, this portfolio can go back and forth between recording a profit or loss. This is due to the histogram being centered around the $100,000 mark.
```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
# each row is a simulated trajectory
# each column is a data
head(sim2)
hist(sim2[,n_days], 25, main="Average Returns - Portfolio 2", xlab="Amount Returned ($USD)", ylab="Frequency")   #give me 25 bins in the histograms
```

This outputs the average USD amount one can receive from this portfolio as well as the average USD amount of the profit/loss yielded. The histogram outputs the range of possible profits/losses. On average, this porfolio returns $99,778.61 which equates to an average loss of $221.37.
```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
# Profit/loss
mean(sim2[,n_days])      #How much money I return from this portfolio and weights
mean(sim2[,n_days] - initial_wealth2)      #How much profit I made from this portfolio
hist(sim2[,n_days]- initial_wealth2, breaks=30, main="Average Profit/Loss - Portfolio 2", xlab="Profit/Loss Returned ($USD)", ylab="Frequency")    #distribution of profit/loss

```

Outputs the 5% value at risk as well as the greatest and least possible value at risks with this iteration.
```{r, echo=FALSE}
# 5% value at risk:
quantile(sim2[,n_days]- initial_wealth2, prob=0.05)
#5% likely to to have a value lower than the amount I put into my portfolio

#99.999999999999999% value at risk:
quantile(sim2[,n_days]- initial_wealth2, prob=0.99999999999999999)
#00.000000000000001% value at risk:
quantile(sim2[,n_days]- initial_wealth2, prob=0.00000000000000001)
```

### Analysis

We decided to pick 6 commodity ETFs as we had some interest in the commodity market and as there has been a general decline recently. Based on the 6 securities that are included, and the results from the code, it is seen that the average profit one will return with this portfolio will be -$221.37 in a 4 week period (20 trading days). However, there is possibility that this portfolio can return as much as $15,028.38 and lose as much as $18,442.35. The range of most likely yields, along with the greatest loss and return amounts back up what we had previously mentioned -- commodities have generally been on the decline. Due to the portfolio holder yielding a loss highlights the fact that commodities are on the decline and are not as strong as they used to be. This is also shown as the magnitude of the portfolio has the potential to lose a far greater amount than what it can potentially gain. The VaR of this portfolio is valued around $7,133.75 meaning that at the 5% confidence level, the portfolio will lose $7,133.75. Each time the simulation runs, these VaR levels change the amount one will return.


### Portfolio 3

Load in stocks that will be included in the portfolio. Make sure that they are gathering data for the past 5 years. Adjusted the stock prices to account for splits, dividends, etc. -- convert to Adjusted Closing Prices.
```{r, include=FALSE}
#Health & Biotech ETFs
#XLV Health Care Select Sector SPDR Fund
#VHT Vanguard Healthcare ETF
#IBB iShares Nasdaq Biotechnology ETF
#IHI iShares U.S. Medical Devices ETF
#XBI SPDR S&P Biotech ETF
mystocks3 = c("XLV", "VHT", "IBB", "IHI", "XBI")
myprices3 = getSymbols(mystocks3, from = "2015-08-08") #from past 5 years

#Adjusting all stocks 
for(ticker in mystocks3) {
  expr3 = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr3))
}
```

These are the variations of returns and top few returns within each of the stocks in the portfolio.
```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
# Look at close-to-close changes -- these are the returns
plot(ClCl(XLVa), main="Adjusted Price Stock Chart - XLV", xlab="Date", ylab="Return (%)")
plot(ClCl(VHTa), main="Adjusted Price Stock Chart - VHT", xlab="Date", ylab="Return (%)")
plot(ClCl(IBBa), main="Adjusted Price Stock Chart - IBB", xlab="Date", ylab="Return (%)")
plot(ClCl(IHIa), main="Adjusted Price Stock Chart - IHI", xlab="Date", ylab="Return (%)")
plot(ClCl(XBIa), main="Adjusted Price Stock Chart - XBI", xlab="Date", ylab="Return (%)")

# Combine all the returns in a matrix
all_returns3 = cbind(ClCl(XLVa), ClCl(VHTa), ClCl(IBBa), ClCl(IHIa), ClCl(XBIa))
head(all_returns3)
```

Returns are calculated by (Price1 - Price0)/Price0. Due to there being no previous data, the first entry has "NA" as the return. This code block removes this from the data set so that results are more accurate.
```{r, echo=FALSE}
# first row is NA because we didn't have a "before" in our data
all_returns3 = as.matrix(na.omit(all_returns3))
N3 = nrow(all_returns3)
```

This shows the pairwise plots (correlation matrix) between all the stocks within the portfolio. By looking at the plots, XLV, VHT, and IHI all have a general linear positive correlation with one another. IBB and XBI both return very obscure correlation plots. We were unable to determine why they output a peculiar plot or what these meant, however, by looking at the stock return charts, both of these stocks have a irregularity which is poteitally the reason for this.
```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
# These returns can be viewed as draws from the joint distribution
# strong correlation, but certainly not Gaussian!  
pairs(all_returns3)
```

Simulating through 5000 iterations of possible returns of the portfolio.
```{r, include=FALSE}
initial_wealth3 = 100000
#repeats 20 day samples 5000 times 
sim3 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth3 = initial_wealth3 #reset total wealth 
  weights3 = c(0.2,0.2,0.2,0.2,0.3)
  holdings3 = weights3 * total_wealth3 #initialize holdings
  n_days = 20 #20 days = 4 weeks
  wealthtracker3 = rep(0, n_days)
  for(today in 1:n_days) { 
    return.today3 = resample(all_returns3, 1, orig.ids=FALSE)
    holdings3 = holdings3 + holdings3*return.today3
    total_wealth3 = sum(holdings3)
    wealthtracker3[today] = total_wealth3
  }
  wealthtracker3
}
```

USD value of the possible returns through the 5000 iterations. The histogram tells us that this portfolio will return, on average, a larger profit compared to the previous two portfolios.
```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
head(sim3)
hist(sim3[,n_days], 25, main="Average Returns - Portfolio 3", xlab="Amount Returned ($USD)", ylab="Frequency") #histogram of simulated final wealth P1
```

This outputs the average USD amount one can receive from this portfolio as well as the average USD amount of the profit/loss yielded. The histogram outputs the range of possible profits/losses. This portfolio returns, on average, $112,755.70 with an average profit of $12,755.70.
```{r, echo=FALSE, fig.height=4, fig.width=8, fig.align='center'}
mean(sim3[,n_days]) #avg return
mean(sim3[,n_days] - initial_wealth3) #avg profit
hist(sim3[,n_days]- initial_wealth3, breaks=30, main="Average Profit/Loss - Portfolio 3", xlab="Profit/Loss Returned ($USD)", ylab="Frequency") #distribution of profit or loss
#majority around 0 ... either expected to have small avg positive or negative returns 
```

Outputs the 5% value at risk as well as the greatest and least possible value at risks with this iteration.
```{r, echo=FALSE}
# 5% value at risk:
quantile(sim3[,n_days]- initial_wealth3, prob=0.05)
#99.999999999999999% value at risk:
quantile(sim3[,n_days]- initial_wealth3, prob=0.99999999999999999)
#00.000000000000001% value at risk:
quantile(sim3[,n_days]- initial_wealth3, prob=0.00000000000000001)
```

### Analysis

Portfolio 3 consists of 5 funds in the Health & Biotech Equities ETFdb Category with the largest value of total assets. We decided to go with these securities as we are in the middle of an pandemic and thought it would be interesting to observe the performance of funds in the healthcare and biotechnology industries. This portfolio is a bit different than the previous two as it is right skewed, meaning that there are outliers for the portfolio holder to yield a large amount. The Health & Biotech ETF portfolio shows that the expected return is $274,932.50, meaning that one will typically make a profit investing of $12,755.70 in these ETFs given their respective weights. There is possibility that this portfolio can return as much as $274,932.50, and lose as much as $16,711.00. The 5% VaR of this portfolio is valued $175.20 meaning that at the 5% confidence level, the portfolio will lose $175.20. Each time the simulation runs, these VaR levels change the amount one will return.

# Exercise 4: Market Segmentation

Our task to is analyze this data as you see fit, and to prepare a concise report for NutrientH20 that identifies any interesting market segments that appear to stand out in their social-media audience.

We performed a PCA analysis to get the components that represents the variance of the data set better. This PCA wasn't scaled because the units are the same (# of tweets for that category) and we want to give more importance to the biggest magnitude.
```{r setup24, include=FALSE}
library(tidyverse)
library(ggplot2)

# Reading data
Z = read.csv('C:/Users/india/Documents/GitHub/STA380/data/social_marketing.csv', header=TRUE, row.names="X")
head(Z)
dim(Z)
cor(Z)

# Getting Frequencies
df = Z/rowSums(Z)

# We don't scale data because units are the same and, here, we want to give more importance to the biggest numbers.
# PCA
pr.out = prcomp(df)
loadings = pr.out$rotation
scores = pr.out$x
```


### Variance

The first component explains 17.51%. This is not bad if we take into account that we have 7882 observations. When we add the second component, the cumulative variance increases to 30.195%.

```{r variance4, echo=FALSE}
par(mfrow=c(1,1))
summary(pr.out)
```

We use scree plots to make a better decision. The elbow in the plot seems to be on PC6 with a cumulative variance explains 58.36%

```{r screeplot4, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
pve=100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow=c(1,2))
plot(pve, type="o", ylab="PVE", xlab="Principal Component",col =" blue ")
par(mfrow=c(1,1))
```

```{r screeplot24, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
par(mfrow=c(1,2))
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="Principal Component ", col =" brown3 ")
par(mfrow=c(1,1))
```

### Analyzing components

The first PC seems to be Appearance v. Fitness.

While chatter might not have been categorized, it could also mean that these tweets were just the users talking with other users. Based on this, maybe they like to go out to shop, look good to share photos, and going for a coffee to talk about non-categorized data.

On the other side, outdoors activities, cooking, personal fitness, and health nutrition seems to describe people that like to take care of themselves. Maybe they go to the gym and run outside in the mornings. To maintain their shape they have to take care of what they eat.

```{r components4, include=FALSE}
loadings_summary = loadings %>%
  as.data.frame() %>%
  rownames_to_column('Classification')
```


For this PC, it seems to be University communities v. (Appearance & Fitness)

Positive values seems to be related to university people aware of news and politics, but also into travel, sports, online gaming, and a pinch of religion. Negative values have characteristics from the previous PC.
```{r pc14, echo=FALSE}
loadings_summary %>%
  select(Classification, PC1) %>%
  arrange(desc(PC1))
```
Based on the scree plot, we thought that 6 PC would be a good number to describe the variance of the data. However, going through the data, we find that the characteristics are repeated, just with some extra appearances in the last PCs like dating, cooking, and family. All of these could be related to previous characteristics. Cooking to health nutrition, dating to chatters, and photo sharing, family with college, or sports. Therefore, we just consider the first two PCs.
```{r pc24, echo=FALSE}
loadings_summary %>%
  select(Classification, PC2) %>%
  arrange(desc(PC2))
```

### Plotting first two PCs

The plots seem to have all three groups that we previously mentioned in a well defined manner. On the left, the fitness people. On the right, the appearance people. On the top, university people.

```{r pcPlot4, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
qplot(scores[,1], scores[,2], color="blue", xlab='Component 1', ylab='Component 2')
```

### K-means

To see if the clusters actually exists, we do k-means with the PCA scores. We use k = 3, since 3 is the numbers of groups we found in PCA.

```{r kmeans4, include=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
km.out=kmeans(scores[,0:2],3,nstart=20)
```

As said before, the three groups are very well defined. On the left (green), the fitness people. On the right (red), the appearance people. On the top (blue), university people.

```{r kmeansplot4, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
plot(scores[,0:2], col=(km.out$cluster +1), main="K-Means Clustering Results with K=3", xlab="Component 1", ylab="Component 2", pch=20, cex=0.5)
```

What would happen with k = 4?

```{r kmeans24, include=FALSE}
km.out2=kmeans(scores[,0:2],4,nstart=20)
```

Fitness people cluster seems to be unchanged. A new group appeared between university people and appearance people. This may be the cluster that started to appear after PC5 with the food, dating, art, automotive, characteristics. However, it seems that the first 3 clusters are good enough to describe the data.

```{r kmeansplot24, echo=FALSE, fig.height = 4, fig.width = 8, fig.align='center'}
plot(scores[,0:2], col=(km.out2$cluster +1), main="K-Means Clustering Results with K=4", xlab="Component 1", ylab="Component 2", pch=20, cex=0.5)
```

To conclude, we found 3 main groups of people with very defined characteristics. Spam and adult columns weren't thrown away. However, they seem not to be important in any PC from 0 to 6. If they had appeared to be an important characteristic and if the company is not related to adult products the columns may have been removed.


# Exercise 5: Author Attribution

NOTE: All of our group members' computers froze and could not complete the knitting process for this exercise. Due to that, all of the code blocks for Exercise 5 are not being evaluated (eval=FALSE) as PCA takes too much time to process/knit. We would like to run the last code block to show the accuracy, however, it does not work without running the previous blocks of code. Due to this abnormality, we are not running anything and instead outputting the hard code behind this exercise and all our descriptions for each hard code block. If you would like to run this code on your own, we recommend you copy and paste this exercise's code into a separate file. When this exercise is in its own, individual file, this does run.

The goal of this exercise is to predict the author of an article on the basis of that article's textual content. We began by installing the necessary libraries.

```{r, include=FALSE}
library(tm) #for text mining
library(tidyverse) #for working with tidy data
library(slam) #for matrices
library(readr) # for reading each text file.
library(stringr) # for extracting the author names.
library(caret) #helps with easy predictive modeling 
library(nnet) #for computing multinomial logistic regression
```

### Reading in the data

The data is stored in 50 articles within a folder for 50 individual authors. There are two directories, a training directory and a test. Each contains the 50 articles for the same 50 authors. Altogether, here are 2,500 articles in the training directory and 2,500 in the test directory. The following code explains how the data was read in. 

```{r, eval=FALSE}

#reading in the training data
dir_train = 'C:/Users/india/Documents/GitHub/STA380/data/ReutersC50/C50train/'
files_train = list.files(dir_train, recursive=T) #vector containing all data in C50 Train folder

#initializing variables to grab train data 
train_i = 1 #index to grab all files for each author
file_train = NULL #empty vector to hold all text 
vector_train = NULL #empty vector to hold all authors names

#getting author and file vectors for the training data
for(x in 1:length(files_train)){ #loop through each entry in files_train vector
  #author_train grabs the author of each file
  author_train = substr(files_train[train_i], start=1, stop=str_locate(files_train[train_i], "/")-1) 
  
  #file_t grabs and combines all text from folder in C50Train directory with author's name
  file_t = read_file(paste0(dir_train, files_train[train_i]))
  
  file_train = c(file_train,file_t) #vector containing all text from each author
  vector_train = c(vector_train,author_train) #vector containing names of all authors
  train_i = train_i + 1 #inc. index to grab next file 
}
#vector_train contains 50 entries of each author's name 
#file_train contains 50 text files for each of the 50 authors

#reading in the test data
dir_test = 'C:/Users/india/Documents/GitHub/STA380/data/ReutersC50/C50test/'
files_test = list.files(dir_test, recursive=T) #vector containing all data in C50Test folder

#initializing same variables to grab test data
test_i = 1 #index to grab all files for each author
file_test = NULL #empty vector to hold all text 
vector_test = NULL #empty vector to hold all authors names

#getting author and file vectors for the test data
for(x in 1:length(files_test)){ #loop through each entry in files_test vector
  #author_test grabs the author of each file
  author_test = substr(files_test[test_i], start=1, stop=str_locate(files_test[test_i], "/")-1) 
  
  #file_test grabs and combines all text from folder in directory w authors name
  f_test = read_file(paste0(dir_test, files_test[test_i]))
  
  file_test = c(file_test,f_test) #vector containing all text from each author
  vector_test = c(vector_test,author_test) #vector containing names of all authors
  test_i = test_i + 1 #inc. index to grab next file 
}

#Checking to make sure we have 50 text files for 50 authors. There should be 50*50=2500 authors and text files for both train and test data
if(length(file_train) == 2500 & length(vector_train) == 2500){print("Train Data Read Successfully")}
if(length(file_test) == 2500 & length(vector_test) == 2500){print("Test Data Read Successfully")}
```

### Preprocessing the data

We pre-processed the data by creating a document term matrix for the train and test data sets. This matrix has column names that are the tokens in each file. Each row corresponds to an individual document. The values in the matrix are the number of times each token appeared in each document. 

The tokens were created by first created a corpus, a giant body of all of the documents. Each of the words in the documents were transformed to lower case and numbers, punctuation, and excess white-space were removed. Additionally, all stop-words from the basic English dictionary were removed, removing all generic words so that we can focus on the unique diction of each quote.

The document term matrix for the training set contained 31,752 terms. We then removed all terms that do not appear in 99% of the documents. This may decrease the accuracy of our model however, it makes our process computationally feasible. As the words removed were very rare (only occurring in 1% of all quotes), it is likely that the cost in accuracy is minimal. Ultimately, the training DTM contained 3,325 words. We pre-processed the test set in exactly the same manner. The resulting test DTM contained also 3,325 words.

```{r, eval=FALSE}
### PRE-PROCESSING: CREATING A DOC-TERM-MATRIX ###

#creating a text mining corpus for all the quotes
train_raw = Corpus(VectorSource(file_train))
#vector source reads in documents one by one
#corpus constructs a corpus consisted of all documents by one author

train_doc = train_raw %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace))          # remove excess white-space

#using "basic English" stop words 
train_doc = tm_map(train_doc, content_transformer(removeWords), stopwords("en"))
#produces warning.. no documents are actually dropped

## create a doc-term-matrix from the corpus
DTM_train = DocumentTermMatrix(train_doc)
DTM_train # some basic summary statistics
#contains 2500 documents with 31,752 terms
#sparsity = 99% indicates we removed terms that only appear in at most 1% of the data
#maximal term length of 36 indicates that the largest number of characters within 1 term is 36


#dropping terms that only occur in one or two documents as there is nothing to learn if a term occurred once.
## Below removes those terms that have count 0 in >99% of docs.  
DTM_train = removeSparseTerms(DTM_train, 0.99)
DTM_train # now ~ 3,325 #Can adjust this percentage if you want to run this code to your own specifications? 
DTM_train1 <- as.matrix(DTM_train)

### PRE-PROCESSING TEST SET

#creating a text mining corpus for all the quotes
test_raw = Corpus(VectorSource(file_test))
#vector source reads in documents one by one
#corpus constructs a corpus consisted of all documents by one author

test_doc = test_raw %>%
  tm_map(content_transformer(tolower))  %>%             # make everything lowercase
  tm_map(content_transformer(removeNumbers)) %>%        # remove numbers
  tm_map(content_transformer(removePunctuation)) %>%    # remove punctuation
  tm_map(content_transformer(stripWhitespace))          # remove excess white-space

#using "basic English" stop words (maybe try again with SMART?)
test_doc = tm_map(test_doc, content_transformer(removeWords), stopwords("en"))
#produces warning.. no documents are actually dropped

## create a doc-term-matrix from the corpus
DTM_test = DocumentTermMatrix(test_doc)
DTM_test # some basic summary statistics
#contains 2500 documents with 31,752 terms
#sparsity = 99% indicates we removed terms that only appear in at most 1% of the data
#maximal term length of 36 indicates that the largest number of characters within 1 term is 36


#dropping terms that only occur in one or two documents as there is nothing to learn if a term occurred once.
## Below removes those terms that have count 0 in >99% of docs.  
DTM_test = removeSparseTerms(DTM_test, 0.99)
DTM_test # now ~ 3,325 #MAYBE ADJUST THIS LATER 
DTM_test1 <- as.matrix(DTM_test)
```

As we want to fit our model on the documents in the training files, the processing procedure becomes a bit complicated as the test files may contain words that our model has not seen before. The test set contained 398 new words.

To help our model process these new words, we added a pseudo-word to the train DTM, labeled "filler_vector" and added a pseudo-count to it. We accomplished this by creating a vector containing all the new words in the test set. Then from the test DTM, we summed up the values in the new word's columns to create a vector that contained the number of new words in each document. We then randomly sampled this vector with replacement to create the "filler_vector". This filler vector will serve as a pseudo-word for all new words in the test set. Our model will be trained using it. As it is filled with a randomly generated sample of the actual counts of new words in the test set, it offers a degree of randomization to our training model so that our test DTM is still unfamiliar and is useful for predictions. 

```{r, eval=FALSE}

## ADD a filler word to DTM_train to fill with new words in test set 

#words in train DTM:
train_words <- colnames(DTM_train1)
test_words <- colnames(DTM_test1)

#finding words that are in test but not train
new_words = setdiff(test_words,train_words) 
length(new_words)#398 new words
length(test_words) #3370 total words 

filler <- c(new_words) #this currently has all new words in test 

new_wordsDF = DTM_test1[,filler]
totalnew <- rowSums(new_wordsDF) #vector for the test set with  how many words are 'new' for each test document.

# sampling with replacement to get the train 'filler' vector - provides degree of randomization in our filler_vector
filler_vector <- sample(totalnew, 2500, replace = TRUE)

#create a dataframe just new "filler" words
test_words <- data.frame(filler_vector)  

#combined DTM matrix 
test_trainDTM <- cbind(DTM_train1, test_words)
```

However, we still needed to pre-process the test set. The test DTM contains words that are in the test set but not the training set. We needed to remove those words and their associated rows out of the test set as they are unfamiliar to the model. We replaced those words with a filler vector, a vector that actually contains the count of new words in each document. We used a randomly sampled version of this vector in our training set to build the model.

```{r, eval=FALSE}
## continuing to pre-process the test set 

#filler is a vector of the words in test but not in train 
`%ni%` <- Negate(`%in%`) #setting this tool to find elements that are not elemnts in another list 
DTM_test1 <- as.data.frame(DTM_test1) #turning test DTM matrix into a dataframe 
DTM_test2 <- DTM_test1[,which(names(DTM_test1) %ni% filler)] #creating a dataframe with words that are in train 

#sanity check: there were 3370 total words in test DTM. We found 398 new words. This data frame should have 
#2972 columns (new words)
if(length(names(DTM_test2))==2972){print("yay success!")}

# "totalnew" contained the count of new words for each test document. 
#create a dataframe with just the count of "new" words in each document 
test_not_new_words <- data.frame(totalnew)  

#combined test DTM matrix 
test_DTM <- cbind(DTM_test2, test_not_new_words)
```

### TFIDF Calculation

Next, we calculated the TF-IDF weights of each word in both the train and test DTMs.

```{r, eval=FALSE}

#TFIDF FOR TRAIN DTM 

N = nrow(test_trainDTM) #number of rows in train DTM with psuedo filler_vector
D = ncol(test_trainDTM) #number of columns 
# TF weights
TF_mat = test_trainDTM /rowSums(test_trainDTM)

# IDF weights
IDF_vec = log(1 + N/colSums(test_trainDTM  > 0))

# TF-IDF weights:
# use sweep to multiply the columns (margin = 2) by the IDF weights
TFIDF_mat = sweep(TF_mat, MARGIN=2, STATS=IDF_vec, FUN="*")  

# spot check an entry
TF_mat[5, 224]
IDF_vec[224]
TFIDF_mat[5,224] == TF_mat[5, 224] * IDF_vec[224]

y <- factor(vector_train) #response vector creates factors of the names of 50 authors

#TFIDF FOR TEST DTM 

y_test <- factor(vector_test) #response vector as name of author

Ntest = nrow(test_DTM)
Dtest = ncol(test_DTM)
# TF weights
TF_mattest = test_DTM /rowSums(test_DTM)

# IDF weights
IDF_vectest = log(1 + N/colSums(test_DTM  > 0))

# TF-IDF weights:
# use sweep to multiply the columns (margin = 2) by the IDF weights
TFIDF_mattest = sweep(TF_mattest, MARGIN=2, STATS=IDF_vectest, FUN="*")  

# spot check an entry
TF_mattest[5, 224]
IDF_vectest[224]
TFIDF_mattest[5,224] == TF_mattest[5, 224] * IDF_vectest[224]
```


### Principal Component Analysis

We then applied principal component analysis to our training data set to reduce the dimensions of our data. This creates linear combinations of original predictor variables to allow us to capture the variance in the data set. We applied PCA to our train data with 10 components, 25 components, and then 40 components. The greater the number of components, the greater the processing power required, and the greater amount of variation captured. PCA with 10 components accounts for 5.48% of the variation in our data, 25 accounts for 9.89%, and 40 accounts for 13.263%. 

The first component captures the maximum variance in the data set and the second captures the second highest variability. By looking at the loadings of the first component, we find that it is contrasting words that are political in nature and relate to the US vs China political discourse with more business and market oriented words. The second component contrasts words related to factories, specifically factories in the MidWestern US, with words also related to business and the markets. 

```{r, eval=FALSE}
### PCA on the TF-IDF weights with 10 PCS
pc_train10 = prcomp(TFIDF_mat, rank=10, scale=TRUE)
loadings = pc_train10$rotation
dim(loadings) #3326 rows with 10 PCs.. expected
scores = pc_train10$x #location
summary(pc_train10) #accounts for 5.48 percent of variation 
```


```{r, eval=FALSE}

comp1 = order(loadings[,1], decreasing=TRUE)
colnames(TFIDF_mat)[head(comp1,25)] 
colnames(TFIDF_mat)[tail(comp1,25)] 
#first component contrasts political words (china/democracy/communism)
#with business words (stock/market/analyst/sales)

comp2 = order(loadings[,2], decreasing=TRUE)
colnames(TFIDF_mat)[head(comp2,25)] 
colnames(TFIDF_mat)[tail(comp2,25)] 
#2nd component contrasts factory (detroit/automaker/strike) 
#with business words (markets/growth/billion)

#merging author name with first 10 PCs
training10 = merge(y, pc_train10$x[,1:10], by="row.names")


### PCA on the TF-IDF weights with 25 PCS (takes longer)
pc_train25 = prcomp(TFIDF_mat, rank=25, scale=TRUE)
loadings2 = pc_train25$rotation
dim(loadings2) #3326 rows with 25 PCs... expected
scores2 = pc_train25$x #location
summary(pc_train25) #25 PC account for 9.89% variation 

### look into what first 2 components indicate 
comp1_25 = order(loadings2[,1], decreasing=TRUE)
colnames(TFIDF_mat)[head(comp1_25,25)] 
colnames(TFIDF_mat)[tail(comp1_25,25)] 
#first component contrasts same as above

comp2_25 = order(loadings2[,2], decreasing=TRUE)
colnames(TFIDF_mat)[head(comp2_25,25)] 
colnames(TFIDF_mat)[tail(comp2_25,25)] 
#2nd component contrasts same as above

#merging author name with first 25 PCs
training25 = merge(y, pc_train25$x[,1:25], by="row.names")

### Looking at 40 components! 
### PCA on the TF-IDF weights with 40 PCS (takes a lot longer)
pc_train40 = prcomp(TFIDF_mat, rank=40, scale=TRUE)
loadings40 = pc_train40$rotation
dim(loadings40) #3326 rows with 40 PCs... expected
scores40 = pc_train40$x #location
summary(pc_train40) #40 PC account for 13.263% variation 

### look into what first 2 components indicate 
comp1_40 = order(loadings40[,1], decreasing=TRUE)
colnames(TFIDF_mat)[head(comp1_40,25)] 
colnames(TFIDF_mat)[tail(comp1_40,25)] 
#first component contrasts same as above

comp2_40 = order(loadings40[,2], decreasing=TRUE)
colnames(TFIDF_mat)[head(comp2_40,25)] 
colnames(TFIDF_mat)[tail(comp2_40,25)] 
#2nd component contrasts same as above

#merging author name with first 25 PCs
training40 = merge(y, pc_train40$x[,1:40], by="row.names")


```

### Applying unsupervised learning to supervised learning

We proceeded by running 3 types of multinomial regressions. The first used the first 10 PCs to predict authorship, the second the first 25 pcs, and the third the first 40 PCs. By looking at the misclassification rate on the training data, we found the model using 10 PCs attained 52.32% accuracy, the model with 25 PCs attained 77.6% accuracy, and the model with 40 PCs attained 91.24% accuracy. Obviously, the model with 40 PC outperformed the other models on the training set. 

```{r, eval=FALSE}

#applying first 10 PC in multinomial regression
train_lm10 <- multinom(x ~ PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10, data = training10)
summary(train_lm10) #residual deviance: 7330.694

## interpreting train_lm10
#getting yhat predictions for train data
pred10 <- predict(train_lm10, newdata = training10, "class")

# Building classification table
ctable10 <- table(training10$x, pred10)

# Calculating accuracy - sum of diagonal elements divided by total obs.. only about 52.32% accurate on train data
round((sum(diag(ctable10))/sum(ctable10))*100,2)

# predicted probabilities for authors
head(pp10 <- fitted(train_lm10))


#applying first 25 PC in multinomial regression
train_lm25 <- multinom(x ~ PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10+PC11+PC12+PC13+PC14
                       +PC15+PC16+PC17+PC18+PC19+PC20+PC21+PC22+PC23+PC24+PC25, 
                       data = training25, MaxNWts=1500)
summary(train_lm25) 

## interpreting train_lm25
#getting yhat predictions for train data
pred25 <- predict(train_lm25, newdata = training25, "class")

# Building classification table
ctable25 <- table(training25$x, pred25)

# Calculating accuracy - sum of diagonal elements divided by total obs.. about 77.6% accurate.. a lot better on train
round((sum(diag(ctable25))/sum(ctable25))*100,2) #inc number of PC's does help! 

# predicted probabilities for authors
head(pp25 <- fitted(train_lm25))


#applying first 40 PC in multinomial regression
train_lm40 <- multinom(x ~ PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10+PC11+PC12+PC13+PC14
                       +PC15+PC16+PC17+PC18+PC19+PC20+PC21+PC22+PC23+PC24+PC25+PC26+PC27+PC28+PC29+PC30+PC31+PC32+PC33+PC34
                       +PC35+PC36+PC37+PC38+PC39+PC40,
                       data = training40, MaxNWts=2200)
summary(train_lm40) 

## interpreting train_lm25
#getting yhat predictions for train data
pred40 <- predict(train_lm40, newdata = training40, "class")

# Building classification table
ctable40 <- table(training40$x, pred40)

# Calculating accuracy - sum of diagonal elements divided by total obs.. about 91.24% accurate.. a lot better on train
#but at a cost to computation speed
round((sum(diag(ctable40))/sum(ctable40))*100,2) #inc number of PC's does help! 

# predicted probabilities for authors
head(pp40 <- fitted(train_lm40))

```

### Making predictions with test data

Before proceeding, we realized that the TFIDF matrix for training data has 3326 words while the test TFIDF has 2973. This is because we removed all 398 new words from the test data set. In order to apply the principal components from our train set on the test set, the columns (the words) of the TFIDF test and train matrices need to be exactly the same. We need to add in all the words that are in the training set and not in test to the test set and give them a TFIDF count of 0, as they do not appear at all. 

```{r, eval=FALSE}

#we need to set the last column containing count of new words in test data set to be of the same name as train.. filler_vector
names(TFIDF_mattest)[length(names(TFIDF_mattest))]<-"filler_vector" 

tfidf_train <- colnames(TFIDF_mat)
tfidf_test <- colnames(TFIDF_mattest)
words2add = c(setdiff(tfidf_train,tfidf_test)) #creating a vector of the words that are different between 2 sets

#creating a matrix of 0 with 2500 rows and number of columns equal to number of new words 2 add 
words2add_matrix <- matrix(0, 2500, length(words2add)) 
#create this into a dataframe
words2addDF <- data.frame(words2add_matrix) 
#specifying the col names as words in train that will be added to test 
colnames(words2addDF) <- words2add

#combined test TFIDF dataframe
TFIDF_test <- cbind(TFIDF_mattest, words2addDF)

```

Now, we're ready to predict authorship of the files in the test directory. We began by applying the PCA from the training set onto the test data. We then made multinomial predictions by applying each of our multinomial models onto test data sets containing 10 train PCs, 25 train PCs, and 40 train PCs. 


```{r, eval=FALSE}
###transform test TFIDF into training PCA

#for 10 PC
test.data10 <- predict(pc_train10, newdata = TFIDF_test)
test.data10 <- as.data.frame(test.data10)
test10 = merge(y_test, test.data10, by="row.names") #merging PC with actual authors

#make multinomial prediction on test data
pred10 <- predict(train_lm10, test10[,3:12], "class") #grabbing class predictions (author predictions)

# Building classification table
ctable_test10 <- table(test10$x, pred10)
# Calculating accuracy - sum of diagonal elements divided by total obs.. 37.88% accurate
round((sum(diag(ctable_test10))/sum(ctable_test10))*100,2)


#for 25 PC
test.data25 <- predict(pc_train25, newdata = TFIDF_test)
test.data25 <- as.data.frame(test.data25)
test25 = merge(y_test, test.data25, by="row.names") #merging PC with actual authors

#make multinomial prediction on test data
pred25 <- predict(train_lm25, test25[,3:27], "class") #grabbing class predictions (author predictions)

# Building classification table
ctable_test25 <- table(test25$x, pred25)
# Calculating accuracy - sum of diagonal elements divided by total obs.. 49.4% accurate
round((sum(diag(ctable_test25))/sum(ctable_test25))*100,2)


#for 10 PC
test.data40 <- predict(pc_train40, newdata = TFIDF_test)
test.data40 <- as.data.frame(test.data40)
test40 = merge(y_test, test.data40, by="row.names") #merging PC with actual authors

#make multinomial prediction on test data
pred40 <- predict(train_lm40, test40[,3:42], "class") #grabbing class predictions (author predictions)

# Building classification table
ctable_test40 <- table(test40$x, pred40)
# Calculating accuracy - sum of diagonal elements divided by total obs.. 50.28 accurate
round((sum(diag(ctable_test40))/sum(ctable_test40))*100,2)

```

### Our results

On the test data, the model with 10 PCs achieved 37.88% accuracy, with 25 PCs achieved 49.4% accuracy, and with 40 PCs achieved 50.28% accuracy. It is rather interesting that though the model with 40 PCs had a training classification rate that was significantly higher than the other models, it barely outperformed the model with 25 PCs on the test data set. In conclusion, the model with 25 PCs is the optimal model. It performs well enough on the test data set while being computationally affordable.

# Exercise 6: Association Rule Mining

```{r, include = FALSE}
library(tidyverse)
library(arules)
library(arulesViz)
library(igraph)
library(data.table)
library(knitr)
```

This reads in the groceries.txt file as a transaction data set and inspects the first 6 baskets.
```{r}
groceries = read.transactions(file = "https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt",
                          format = c("basket"), sep = ",")

arules::inspect(head(groceries))
```

With a threshold of support=0.05, the most frequent items in the data set are selected and the top 15 are shown. They are sorted by decreasing support.
```{r}
FrequentItems = eclat(groceries, parameter = list(support = 0.05))
FrequentItems <- sort(FrequentItems, by = "support", decreasing = TRUE)
arules::inspect(head(FrequentItems, 15))
```


This plot shows the absolute frequency of the top 15 groceries in the data set (outputted above).
```{r, echo = FALSE, fig.height=4, fig.width=8, fig.align='center'}
itemFrequencyPlot(groceries, topN = 15, type = "absolute",
                  main = "Item Frequency")
```


This plot shows the relative frequency of the top 15 groceries in the data set (outputted above).
```{r, echo = FALSE, fig.height=4, fig.width=8, fig.align='center'}
itemFrequencyPlot(groceries, topN = 15, type = "relative",
                  main = "Item Frequency")

```


A total of 32791 rules are created when using support=0.001 and confidence=0.1. Below only show the top 6 association rules.
```{r}
groceryRules <- apriori(groceries, parameter = list(support = 0.001, confidence = 0.1))
arules::inspect(head(groceryRules))
```


This creates a new rules matrix that sorts grocery rules by decreasing support. Again, the first 15 association rules of this new rules matrix are shown.
```{r, fig.align='center'}
rulesSupport <- sort(groceryRules, by = "support", decreasing = TRUE)
arules::inspect(head(rulesSupport, 15))
```


This creates a new rules matrix that sorts grocery rules by decreasing confidence Again, the first 15 association rules of this new rules matrix are shown.
```{r, fig.align='center'}
rulesConfidence <- sort(groceryRules, by = "confidence", decreasing = TRUE)
arules::inspect(head(rulesConfidence, 15))
```


This creates a new rules matrix that sorts grocery rules by decreasing lift Again, the first 15 association rules of this new rules matrix are shown.
```{r, fig.align='center'}
rulesLift <- sort(groceryRules, by = "lift", decreasing = TRUE)
arules::inspect(head(rulesLift, 15))
```


There are 28 different association rules where confidence = 1. Notice that the support remains relatively low, hovering around 0.001. Lift for these rules tends to be about 4 or 5.
```{r, fig.align='center'}
arules::inspect(subset(rulesConfidence, subset = confidence == 1))
```


There are 20 different association rules where lift > 15. Notice that support is also relatively low, hovering around 0.01. Confidence ranges between 0.12 and 0.65.
```{r, fig.align='center'}
arules::inspect(subset(rulesLift, subset = lift > 15))
```


As seen from the inspection of subsets above, the consequent(s) of 100% confidence rules are 'whole milk' and 'other vegetables'. This makes sense seeing that people, in general, consume milk and vegetables often as these are high in demand and are constantly used with many meals throughout the day.

The consequent(s) of high lift rules include liquor, instant food products, rice, hamburger meat, wine, and popcorn among others. These seem to be true as they all have a general connection with one another. When people are BBQing, instant food products, hamburgers, meat, and alcohol are generally in the mix.

The plot confirms what was stated above. Individual data points are shaded by confidence, and the most confidence occurs at low support and low to medium levels of lift. 
```{r, echo = FALSE, fig.height=4, fig.width=8, fig.align='center'}
plot(groceryRules, measure = c("support", "lift"), shading = "confidence")
```


A subset of rules with confidence > 0.1 and support > 0.01 was created. We inspect the first 6 association rules of this new rules matrix which contains a total of 435 rules.
```{r}
sub1 = subset(groceryRules, subset = confidence > 0.1 & support > 0.01)
arules::inspect(head(sub1))
```


A graph of 50 rules which are sorted by the highest lift based on the subset given above: confidence > 0.1 and support> 0.01. This also coincides with the ides that 'whole milk' and 'other vegetables' are high in support as they are in the center of this graph. Many other categories are associated with them.
```{r, echo = FALSE, fig.height=4, fig.width=8, fig.align='center'}
plot(head(sub1, 50, by = 'lift'), method = 'graph')
```


There is a graphml file containing 1084 nodes and 3907 edges of groceryRules. The file will be uploaded onto GitHub. We tried to have the image upload within the .rmd file, however, we encountered many errors. Instead, we have uploaded it to GitHub in a separate file (same repository as this file) as well as included the image in the email sent. Some characteristics of the network are listed below.
Network diameter =  14
Graph Density    =  0.003
Modularity       =  0.331
Average Path Length  =  4.537